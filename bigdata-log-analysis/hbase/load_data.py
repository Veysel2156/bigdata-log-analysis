"""
HBase Veri YÃ¼kleyici - TAMAMLANMIÅ (Rank HatasÄ± DÃ¼zeltildi)
"""
import happybase
from pyspark.sql import SparkSession
import sys
import time
# schema.py dosyasÄ±ndan ÅŸemayÄ± Ã§ekiyoruz
from schema import HBASE_SCHEMA

# Ayarlar
HBASE_HOST = "hbase"
HBASE_PORT = 9090
MINIO_OPTS = {
    "endpoint": "http://minio:9000",
    "access_key": "minioadmin",
    "secret_key": "minioadmin123"
}

def connect_hbase():
    print(f"ğŸ“¡ HBase'e baÄŸlanÄ±lÄ±yor ({HBASE_HOST})...")
    try:
        conn = happybase.Connection(host=HBASE_HOST, port=HBASE_PORT, timeout=10000)
        conn.open()
        print("âœ… BaÄŸlantÄ± baÅŸarÄ±lÄ±")
        return conn
    except Exception as e:
        print(f"âŒ BaÄŸlantÄ± hatasÄ±: {e}")
        sys.exit(1)

def create_tables(conn):
    try:
        existing = set(t.decode('utf-8') for t in conn.tables())
        for t_name, schema in HBASE_SCHEMA.items():
            if t_name not in existing:
                print(f"ğŸ”¨ Tablo oluÅŸturuluyor: {t_name}")
                conn.create_table(t_name, {cf: dict() for cf in schema['cf']})
    except Exception as e:
        print(f"âš ï¸ Tablo oluÅŸturma hatasÄ±: {e}")

def get_spark():
    return SparkSession.builder \
        .appName("HBaseLoader") \
        .config("spark.executor.memory", "1g") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262") \
        .config("spark.hadoop.fs.s3a.endpoint", MINIO_OPTS["endpoint"]) \
        .config("spark.hadoop.fs.s3a.access.key", MINIO_OPTS["access_key"]) \
        .config("spark.hadoop.fs.s3a.secret.key", MINIO_OPTS["secret_key"]) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
        .getOrCreate()

def load_table(spark, conn, table_name, path, row_key_col, col_family):
    print(f"ğŸ“¥ {table_name} yÃ¼kleniyor...")
    try:
        # Veriyi oku
        df = spark.read.parquet(path)
        rows = df.collect()
        
        if not rows:
            print(f"   âš ï¸ {path} altÄ±nda veri bulunamadÄ±, atlanÄ±yor.")
            return

        table = conn.table(table_name)
        batch = table.batch()
        
        count = 0
        # DÃœZELTME BURADA: enumerate ile sÄ±ralamayÄ± biz veriyoruz
        for i, row in enumerate(rows):
            # Row Key belirle
            if table_name == "top_users":
                # Veri zaten sÄ±ralÄ± olduÄŸu iÃ§in index'i (i) rank olarak kullanÄ±yoruz
                rank = i + 1 
                rk = f"user_{rank}_{row['user_id']}".encode()
                
                # Rank bilgisini kolonlara da ekleyelim ki dashboard'da gÃ¶rÃ¼nsÃ¼n
                batch.put(rk, {f"{col_family}:rank".encode(): str(rank).encode()})
                
            elif table_name == "hourly_traffic":
                 rk = f"hour_{row['hour']}".encode()
            else:
                rk = str(row[row_key_col]).encode()
            
            # DiÄŸer kolonlarÄ± ekle
            data = {}
            for col_name in row.asDict():
                if col_name != row_key_col:
                    val = str(row[col_name]).encode()
                    data[f"{col_family}:{col_name}".encode()] = val
            
            batch.put(rk, data)
            count += 1
        
        batch.send()
        print(f"âœ… {count} satÄ±r yÃ¼klendi.")
    except Exception as e:
        print(f"âš ï¸  Hata ({table_name}): {e}")

def main():
    conn = connect_hbase()
    create_tables(conn)
    
    spark = get_spark()
    spark.sparkContext.setLogLevel("ERROR")
    
    base = "s3a://analytics/results"
    
    # Verileri YÃ¼kle
    load_table(spark, conn, "response_metrics", f"{base}/response_time_metrics", "endpoint", "metrics")
    load_table(spark, conn, "service_errors", f"{base}/service_errors", "service", "stats")
    load_table(spark, conn, "region_traffic", f"{base}/region_traffic", "region", "traffic")
    load_table(spark, conn, "hourly_traffic", f"{base}/hourly_traffic", "hour", "traffic")
    load_table(spark, conn, "top_users", f"{base}/top_users", "user_id", "user")
    
    conn.close()
    spark.stop()

if __name__ == "__main__":
    main()