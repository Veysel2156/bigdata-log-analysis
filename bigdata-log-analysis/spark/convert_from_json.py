"""
JSON'dan Parquet'ye DÃ¶nÃ¼ÅŸtÃ¼rme - TAMAMLANMIÅž
"""
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
from pyspark.sql.functions import to_timestamp
import time

# MinIO AyarlarÄ±
MINIO_CONF = {
    "endpoint": "http://minio:9000",
    "access_key": "minioadmin",
    "secret_key": "minioadmin123"
}

def main():
    print("ðŸš€ DÃ–NÃœÅžTÃœRME BAÅžLIYOR...")
    
    # Spark Oturumu
    spark = SparkSession.builder \
        .appName("FormatConverter") \
        .config("spark.executor.memory", "2g") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262") \
        .config("spark.hadoop.fs.s3a.endpoint", MINIO_CONF["endpoint"]) \
        .config("spark.hadoop.fs.s3a.access.key", MINIO_CONF["access_key"]) \
        .config("spark.hadoop.fs.s3a.secret.key", MINIO_CONF["secret_key"]) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
        .getOrCreate()
        
    spark.sparkContext.setLogLevel("ERROR")

    # Åžema TanÄ±mÄ±
    schema = StructType([
        StructField("timestamp", StringType(), True),
        StructField("service", StringType(), True),
        StructField("endpoint", StringType(), True),
        StructField("level", StringType(), True),
        StructField("user_id", IntegerType(), True),
        StructField("ip", StringType(), True),
        StructField("region", StringType(), True),
        StructField("response_time", IntegerType(), True),
        StructField("status_code", IntegerType(), True),
        StructField("error_code", StringType(), True),
        StructField("message", StringType(), True),
        StructField("warn_detail", StringType(), True) 
    ])

    # 1. JSON Oku
    print("ðŸ“¥ JSON verisi okunuyor (MinIO)...")
    start = time.time()
    df = spark.read.schema(schema).json("s3a://logs/*.json")
    df = df.withColumn("timestamp", to_timestamp("timestamp"))
    
    count = df.count()
    print(f"âœ… {count:,} kayÄ±t bulundu. (SÃ¼re: {time.time()-start:.2f}sn)")

    # 2. Parquet Yaz
    print("ðŸ“¦ Parquet formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
    start = time.time()
    # Analytics klasÃ¶rÃ¼ altÄ±na yazÄ±yoruz ki analiz scripti bulabilsin
    df.write.mode("overwrite").parquet("s3a://parquet/logs")
    print(f"âœ… DÃ¶nÃ¼ÅŸtÃ¼rme tamamlandÄ±. (SÃ¼re: {time.time()-start:.2f}sn)")

    spark.stop()

if __name__ == "__main__":
    main()